{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fac0911",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Pour commencer ce projet, on a accès à une base de données contenant une certaine quantité de clients, chacun identifié par un ID unique? Cette base de données est répartie en multiples fichiers contenant diverses informations sur ces clients. La première étape va donc être de traiter ces données afin de pouvoir les utiliser au mieux, ainsi que de regrouper les tables afin d'obtenir une ligne unique par client dans notre dataset.\n",
    "\n",
    "Pour accélérer le travail, comme conseillé sur l'énoncé du projet j'ai repris un feature engineering pré éxistant via la compétition Kaggle originale que j'ai légèrement adapté et modifié pour une meilleure compréhension et utilisation. Le script original peut être trouvé [ici](https://www.kaggle.com/code/jsaguiar/lightgbm-with-simple-features/script). L'intérêt de ce feature engineering est d'agréger de nombreuses features en gardant les moyennes/mean/max à travers divers groupby afin d'extraire un maximum d'information pour notre modèle. Il vient également créer quelques nouvelles features à l'aide de pré-existantes. Enfin, il traite toutes les tables une par une, puis les merge ensemble afin d'obtenir un dataset propre avec une ligne unique par ID_CLIENT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a35da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On importe les différentes librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import warnings\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Fonction pour calculer les temps de traitement\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721c0cb5",
   "metadata": {},
   "source": [
    "On commence par créer une fonction qui applique un one hot encoder sur les features catégorielles d'un dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b89a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui applique un one hot encoding avec get_dummies\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a5bef2",
   "metadata": {},
   "source": [
    "On rappelle que nos données sont sous forme de database avec plusieurs tables, on va donc maintenant les traiter une à une."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbfb774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess les données de train et test\n",
    "def application_train_test(num_rows=None, nan_as_category=False):\n",
    "    # Importer les données\n",
    "    df = pd.read_csv('Data/application_train.csv', nrows=num_rows)\n",
    "    test_df = pd.read_csv('Data/application_test.csv', nrows=num_rows)\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
    "\n",
    "    # Joindre les deux tables afin de tout traiter d'un coup\n",
    "    df = pd.concat([df, test_df]).reset_index(drop=True)\n",
    "    # Retirer les lignes avec Code_gender = XNA\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "\n",
    "    # Encoder les variables catégoriques binaires\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    # Encoder les variables catégoriques avec One Hot Encoder\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "\n",
    "    # Valeurs positives (=365243 par défaut) pour DAYS_EMPLOYED → NaN\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "    # Nouvelles features (pourcentages)\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "897296ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess les données de bureau.csv et bureau_balance.csv\n",
    "def bureau_and_balance(num_rows=None, nan_as_category=True):\n",
    "    # Importer les données\n",
    "    bureau = pd.read_csv('Data/bureau.csv', nrows=num_rows)\n",
    "    bb = pd.read_csv('Data/bureau_balance.csv', nrows=num_rows)\n",
    "    # Transformer les données catégoriques avec One Hot Encoder pour les deux tables\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "\n",
    "    # Agrégations sur bureau_balance\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    \n",
    "    # Merge des deux tables\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace=True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "\n",
    "    # Traitement des features numériques\n",
    "    # Dict avec les valeurs qu'on garde pour chaque agrégation de feature\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    # Traitement des features catégoriques\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat:\n",
    "        cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "\n",
    "    # Agrégations sur bureau sur toutes les colonnes avec les dicts définis précédemment\n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "\n",
    "    # On sépare les valeurs selon si le prêt est ACTIF ou FERME\n",
    "    # Garder les crédits actifs\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    # Agrégation et rename pour différencier les features\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    # Merge avec la table finale bureau_agg\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "\n",
    "    # Même chose que précédemment pour différencier les crédits FERMES\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49f3ca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess previous_applications.csv\n",
    "def previous_applications(num_rows=None, nan_as_category=True):\n",
    "    # Import les données et OHE sur les features catégoriques\n",
    "    prev = pd.read_csv('Data/previous_application.csv', nrows=num_rows)\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category)\n",
    "    # Remplacer les valeurs positives (365243) par des NaN\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n",
    "    # Nouvelle feature : valeur demandée / valeur reçue en pourcentage\n",
    "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    # On remplace les valeurs infinies si AMT_CREDIT est nul\n",
    "    prev['APP_CREDIT_PERC'].replace(np.inf,0,inplace=True)\n",
    "    \n",
    "    # Agrégations pour les features numériques\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    # Prendre la moyenne pour les features catégoriques\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "\n",
    "    # Agrégation pour chaque demande\n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    # On vient encore une fois séparer les données entre les demandes approuvées et les demandes refusées\n",
    "    # Anciennes demandes : Agrégation features numériques pour les demandes approuvées\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    # Anciennes demandes : Agrégation features numériques pour les demandes refusées\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "    return prev_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7079b027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess POS_CASH_balance.csv\n",
    "def pos_cash(num_rows=None, nan_as_category=True):\n",
    "    pos = pd.read_csv('Data/POS_CASH_balance.csv', nrows=num_rows)\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category)\n",
    "    # Agrégations des features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "\n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    # Quantité de comptes pos cash\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "    del pos\n",
    "    gc.collect()\n",
    "    return pos_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d05ab81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess installments_payments.csv\n",
    "def installments_payments(num_rows=None, nan_as_category=True):\n",
    "    ins = pd.read_csv('Data/installments_payments.csv', nrows=num_rows)\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category)\n",
    "    # Nouvelles features : Pourcentage payé et différence pour chaque versement\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    # Remplacer les valeurs infinies si AMT_INSTALMENT est nul\n",
    "    ins['PAYMENT_PERC'].replace(np.inf,0,inplace=True)\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "    \n",
    "    # Nouvelles features : Paiement en avance / en retard (Days before and days past), en gardant les valeurs positives\n",
    "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    # Agrégations des features\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    # Total des versements\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    return ins_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6a1cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess credit_card_balance.csv\n",
    "def credit_card_balance(num_rows=None, nan_as_category=True):\n",
    "    cc = pd.read_csv('Data/credit_card_balance.csv', nrows=num_rows)\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category)\n",
    "    # General aggregations\n",
    "    cc.drop(['SK_ID_PREV'], axis=1, inplace=True)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    # Compte le nombre de paiements par ID\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37092ab4",
   "metadata": {},
   "source": [
    "Maintenant qu'on a créer des fonctions quit traitent les différentes tables une à une, on va définir une fonction qui run toutes les fonctions et merge les datasets retournés en un seul sur le SK_ID_CURR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "536e99db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée ensuite une fonction main() qui lance toutes les fonctions définies précédemment\n",
    "def feature_engin(debug=False):\n",
    "    num_rows = 10000 if debug else None\n",
    "    df = application_train_test(num_rows)\n",
    "    with timer(\"Process bureau and bureau_balance\"):\n",
    "        bureau = bureau_and_balance(num_rows)\n",
    "        print(\"Bureau df shape:\", bureau.shape)\n",
    "        df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "        del bureau\n",
    "        gc.collect()\n",
    "    with timer(\"Process previous_applications\"):\n",
    "        prev = previous_applications(num_rows)\n",
    "        print(\"Previous applications df shape:\", prev.shape)\n",
    "        df = df.join(prev, how='left', on='SK_ID_CURR')\n",
    "        del prev\n",
    "        gc.collect()\n",
    "    with timer(\"Process POS-CASH balance\"):\n",
    "        pos = pos_cash(num_rows)\n",
    "        print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "        df = df.join(pos, how='left', on='SK_ID_CURR')\n",
    "        del pos\n",
    "        gc.collect()\n",
    "    with timer(\"Process installments payments\"):\n",
    "        ins = installments_payments(num_rows)\n",
    "        print(\"Installments payments df shape:\", ins.shape)\n",
    "        df = df.join(ins, how='left', on='SK_ID_CURR')\n",
    "        del ins\n",
    "        gc.collect()\n",
    "    with timer(\"Process credit card balance\"):\n",
    "        cc = credit_card_balance(num_rows)\n",
    "        print(\"Credit card balance df shape:\", cc.shape)\n",
    "        df = df.join(cc, how='left', on='SK_ID_CURR')\n",
    "        del cc\n",
    "        gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0659fd2d",
   "metadata": {},
   "source": [
    "Avec toutes ces fonctions on va pouvoir créer un dataset entier, comportant une ligne unique pour chaque demande de prêt avec toutes ses caractéristiques. On va tout de même effectuer une séparation, car on a rassemblé les données du set d'entraînement avec celles du set de test pour la compétition Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e4d3a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 307511, test samples: 48744\n",
      "Bureau df shape: (305811, 116)\n",
      "Process bureau and bureau_balance - done in 13s\n",
      "Previous applications df shape: (338857, 249)\n",
      "Process previous_applications - done in 13s\n",
      "Pos-cash balance df shape: (337252, 18)\n",
      "Process POS-CASH balance - done in 7s\n",
      "Installments payments df shape: (339587, 26)\n",
      "Process installments payments - done in 16s\n",
      "Credit card balance df shape: (103558, 141)\n",
      "Process credit card balance - done in 13s\n",
      "Full model run - done in 64s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Full model run\"):\n",
    "    df = feature_engin()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af293d11",
   "metadata": {},
   "source": [
    "Après toutes ces agrégations, on a tout de même de nombreuses colonnes dans notre dataset qui sont remplies en grande partie de valeurs manquantes. Retirons donc une partie de ces features.\n",
    "\n",
    "On va de plus séparer le set d'entraînement afin d'obtenir un set de validation qui servira à contrôler les résultats de la cross validation, le set de test servant uniquement pour obtenir une note via Kaggle. Le travail suivant se fera ensuite à l'aide d'une sample du dataset, afin de réduire les temps de traitement de notre analyse et des cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b166a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprime les features avec trop de valeurs manquantes\n",
    "def empty_features(data) :\n",
    "    features = data.loc[:,data.isna().mean()>0.75].columns\n",
    "    data.drop(features, inplace=True, axis=1) #35 colonnes\n",
    "    return data\n",
    "\n",
    "# Sépare le dataset en 2 sets et prend un échantillon\n",
    "def sample_train_test(data) :\n",
    "    # Récupérer les sets train et test originaux\n",
    "    df_test = data[data['TARGET'].isna()].reset_index(drop=True)\n",
    "    df_train = data[~data['TARGET'].isna()].reset_index(drop=True) \n",
    "    \n",
    "    # Echantillonage\n",
    "    df_sample = df_train.groupby('TARGET').sample(frac=0.05, random_state=10).reset_index(drop=True)\n",
    "    \n",
    "    return df_test, df_train, df_sample\n",
    "\n",
    "\n",
    "df = empty_features(df)\n",
    "df_test, df_train, df_sample = sample_train_test(df)\n",
    "\n",
    "# On sépare le set de train pour créer un set de validation\n",
    "features_train = [x for x in df_train.columns if x not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV']]\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    df_sample[features_train], \n",
    "    df_sample['TARGET'], test_size=0.25, random_state=10, stratify=df_sample['TARGET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f7884",
   "metadata": {},
   "source": [
    "Maintenant qu'on a un dataset utilisable, on va pouvoir essayer divers modèles afin de comparer les résultats obtenus. On prendra ensuite le modèle le plus performant et on viendra utiliser des algorithmes de cross validation afin d'optimiser ses hyperparamètres.\n",
    "\n",
    "Cependant, pour comparer ces modèles, ainsi que pour optimiser les paramètres, il va nous falloir utiliser un scorer. Pour cela on va utiliser des scores habituels, tels que l'accuracy ou le F1 score, ou bien l'AUC car c'est le score utilisé dans la compétition Kaggle. On va aussi créer notre propre scorer, qu'on appelera score métier, qui viendra calculer les pertes engendrées par les différentes erreurs de prédiction.\n",
    "\n",
    "Pour le calcul de ce score, on considère qu'un Faux Négatif (C'est à dire un mauvais client prédit comme un bon client) coûte 10 fois plus cher qu'un Faux Positif (Bon client prédit comme mauvais client)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0e39d96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def score_metier(y_true,y_pred,seuil=0.5) :\n",
    "    y_true.reset_index(inplace=True,drop=True)\n",
    "    error = 0\n",
    "    for i in range (len(y_true)) :\n",
    "        # Si la probabilité est suffisante le prêt est refusé\n",
    "        if y_pred[i] > seuil :\n",
    "            #Prêt refusé alors que le client aurait remboursé = manque à gagner\n",
    "            if y_true[i] == 0:\n",
    "                error += 1\n",
    "        else :\n",
    "            #Prêt accepté alors que le client ne peut pas rembourser = perte d'argent\n",
    "            if y_true[i] == 1:\n",
    "                error += 10\n",
    "    return error\n",
    "\n",
    "metier_scorer = make_scorer(score_metier, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "74211cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91926112, 0.08073888],\n",
       "       [0.91926112, 0.08073888],\n",
       "       [0.91926112, 0.08073888],\n",
       "       ...,\n",
       "       [0.91926112, 0.08073888],\n",
       "       [0.91926112, 0.08073888],\n",
       "       [0.91926112, 0.08073888]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier()\n",
    "dummy.fit(x_train,y_train)\n",
    "dummy.predict_proba(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef26f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
