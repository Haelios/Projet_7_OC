{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fac0911",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Pour commencer ce projet, on a accès à une base de données contenant une certaine quantité de clients, chacun identifié par un ID unique? Cette base de données est répartie en multiples fichiers contenant diverses informations sur ces clients. La première étape va donc être de traiter ces données afin de pouvoir les utiliser au mieux, ainsi que de regrouper les tables afin d'obtenir une ligne unique par client dans notre dataset.\n",
    "\n",
    "Pour accélérer le travail, comme conseillé sur l'énoncé du projet j'ai repris un feature engineering pré éxistant via la compétition Kaggle originale que j'ai légèrement adapté et modifié pour une meilleure compréhension et utilisation. Le script original peut être trouvé [ici](https://www.kaggle.com/code/jsaguiar/lightgbm-with-simple-features/script). L'intérêt de ce feature engineering est d'agréger de nombreuses features en gardant les moyennes/mean/max à travers divers groupby afin d'extraire un maximum d'information pour notre modèle. Il vient également créer quelques nouvelles features à l'aide de pré-existantes. Enfin, il traite toutes les tables une par une, puis les merge ensemble afin d'obtenir un dataset propre avec une ligne unique par ID_CLIENT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a35da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On importe les différentes librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import warnings\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Fonction pour calculer les temps de traitement\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721c0cb5",
   "metadata": {},
   "source": [
    "On commence par créer une fonction qui applique un one hot encoder sur les features catégorielles d'un dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b89a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui applique un one hot encoding avec get_dummies\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a5bef2",
   "metadata": {},
   "source": [
    "On rappelle que nos données sont sous forme de database avec plusieurs tables, on va donc maintenant les traiter une à une."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbfb774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess les données de train et test\n",
    "def application_train_test(num_rows=None, nan_as_category=False):\n",
    "    # Importer les données\n",
    "    df = pd.read_csv('Data/application_train.csv', nrows=num_rows)\n",
    "    test_df = pd.read_csv('Data/application_test.csv', nrows=num_rows)\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
    "\n",
    "    # Joindre les deux tables afin de tout traiter d'un coup\n",
    "    df = pd.concat([df, test_df]).reset_index(drop=True)\n",
    "    # Retirer les lignes avec Code_gender = XNA\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "\n",
    "    # Encoder les variables catégoriques binaires\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    # Encoder les variables catégoriques avec One Hot Encoder\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "\n",
    "    # Valeurs positives (=365243 par défaut) pour DAYS_EMPLOYED → NaN\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "    # Nouvelles features (pourcentages)\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "897296ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess les données de bureau.csv et bureau_balance.csv\n",
    "def bureau_and_balance(num_rows=None, nan_as_category=True):\n",
    "    # Importer les données\n",
    "    bureau = pd.read_csv('Data/bureau.csv', nrows=num_rows)\n",
    "    bb = pd.read_csv('Data/bureau_balance.csv', nrows=num_rows)\n",
    "    # Transformer les données catégoriques avec One Hot Encoder pour les deux tables\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "\n",
    "    # Agrégations sur bureau_balance\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    \n",
    "    # Merge des deux tables\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace=True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "\n",
    "    # Traitement des features numériques\n",
    "    # Dict avec les valeurs qu'on garde pour chaque agrégation de feature\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    # Traitement des features catégoriques\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat:\n",
    "        cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "\n",
    "    # Agrégations sur bureau sur toutes les colonnes avec les dicts définis précédemment\n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "\n",
    "    # On sépare les valeurs selon si le prêt est ACTIF ou FERME\n",
    "    # Garder les crédits actifs\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    # Agrégation et rename pour différencier les features\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    # Merge avec la table finale bureau_agg\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "\n",
    "    # Même chose que précédemment pour différencier les crédits FERMES\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49f3ca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess previous_applications.csv\n",
    "def previous_applications(num_rows=None, nan_as_category=True):\n",
    "    # Import les données et OHE sur les features catégoriques\n",
    "    prev = pd.read_csv('Data/previous_application.csv', nrows=num_rows)\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category)\n",
    "    # Remplacer les valeurs positives (365243) par des NaN\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n",
    "    # Nouvelle feature : valeur demandée / valeur reçue en pourcentage\n",
    "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    # On remplace les valeurs infinies si AMT_CREDIT est nul\n",
    "    prev['APP_CREDIT_PERC'].replace(np.inf,0,inplace=True)\n",
    "    \n",
    "    # Agrégations pour les features numériques\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    # Prendre la moyenne pour les features catégoriques\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "\n",
    "    # Agrégation pour chaque demande\n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    # On vient encore une fois séparer les données entre les demandes approuvées et les demandes refusées\n",
    "    # Anciennes demandes : Agrégation features numériques pour les demandes approuvées\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    # Anciennes demandes : Agrégation features numériques pour les demandes refusées\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "    return prev_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7079b027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess POS_CASH_balance.csv\n",
    "def pos_cash(num_rows=None, nan_as_category=True):\n",
    "    pos = pd.read_csv('Data/POS_CASH_balance.csv', nrows=num_rows)\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category)\n",
    "    # Agrégations des features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "\n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    # Quantité de comptes pos cash\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "    del pos\n",
    "    gc.collect()\n",
    "    return pos_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d05ab81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess installments_payments.csv\n",
    "def installments_payments(num_rows=None, nan_as_category=True):\n",
    "    ins = pd.read_csv('Data/installments_payments.csv', nrows=num_rows)\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category)\n",
    "    # Nouvelles features : Pourcentage payé et différence pour chaque versement\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    # Remplacer les valeurs infinies si AMT_INSTALMENT est nul\n",
    "    ins['PAYMENT_PERC'].replace(np.inf,0,inplace=True)\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "    \n",
    "    # Nouvelles features : Paiement en avance / en retard (Days before and days past), en gardant les valeurs positives\n",
    "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    # Agrégations des features\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    # Total des versements\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    return ins_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6a1cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess credit_card_balance.csv\n",
    "def credit_card_balance(num_rows=None, nan_as_category=True):\n",
    "    cc = pd.read_csv('Data/credit_card_balance.csv', nrows=num_rows)\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category)\n",
    "    # General aggregations\n",
    "    cc.drop(['SK_ID_PREV'], axis=1, inplace=True)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    # Compte le nombre de paiements par ID\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37092ab4",
   "metadata": {},
   "source": [
    "Maintenant qu'on a créer des fonctions quit traitent les différentes tables une à une, on va définir une fonction qui run toutes les fonctions et merge les datasets retournés en un seul sur le SK_ID_CURR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "536e99db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée ensuite une fonction main() qui lance toutes les fonctions définies précédemment\n",
    "def feature_engin(debug=False):\n",
    "    num_rows = 10000 if debug else None\n",
    "    df = application_train_test(num_rows)\n",
    "    with timer(\"Process bureau and bureau_balance\"):\n",
    "        bureau = bureau_and_balance(num_rows)\n",
    "        print(\"Bureau df shape:\", bureau.shape)\n",
    "        df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "        del bureau\n",
    "        gc.collect()\n",
    "    with timer(\"Process previous_applications\"):\n",
    "        prev = previous_applications(num_rows)\n",
    "        print(\"Previous applications df shape:\", prev.shape)\n",
    "        df = df.join(prev, how='left', on='SK_ID_CURR')\n",
    "        del prev\n",
    "        gc.collect()\n",
    "    with timer(\"Process POS-CASH balance\"):\n",
    "        pos = pos_cash(num_rows)\n",
    "        print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "        df = df.join(pos, how='left', on='SK_ID_CURR')\n",
    "        del pos\n",
    "        gc.collect()\n",
    "    with timer(\"Process installments payments\"):\n",
    "        ins = installments_payments(num_rows)\n",
    "        print(\"Installments payments df shape:\", ins.shape)\n",
    "        df = df.join(ins, how='left', on='SK_ID_CURR')\n",
    "        del ins\n",
    "        gc.collect()\n",
    "    with timer(\"Process credit card balance\"):\n",
    "        cc = credit_card_balance(num_rows)\n",
    "        print(\"Credit card balance df shape:\", cc.shape)\n",
    "        df = df.join(cc, how='left', on='SK_ID_CURR')\n",
    "        del cc\n",
    "        gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0659fd2d",
   "metadata": {},
   "source": [
    "Avec toutes ces fonctions on va pouvoir créer un dataset entier, comportant une ligne unique pour chaque demande de prêt avec toutes ses caractéristiques. On va tout de même effectuer une séparation, car on a rassemblé les données du set d'entraînement avec celles du set de test pour la compétition Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e4d3a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 307511, test samples: 48744\n",
      "Bureau df shape: (305811, 116)\n",
      "Process bureau and bureau_balance - done in 13s\n",
      "Previous applications df shape: (338857, 249)\n",
      "Process previous_applications - done in 13s\n",
      "Pos-cash balance df shape: (337252, 18)\n",
      "Process POS-CASH balance - done in 7s\n",
      "Installments payments df shape: (339587, 26)\n",
      "Process installments payments - done in 16s\n",
      "Credit card balance df shape: (103558, 141)\n",
      "Process credit card balance - done in 12s\n",
      "Full model run - done in 63s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Full model run\"):\n",
    "    df = feature_engin()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af293d11",
   "metadata": {},
   "source": [
    "Après toutes ces agrégations, on a tout de même de nombreuses colonnes dans notre dataset qui sont remplies en grande partie de valeurs manquantes. Retirons donc une partie de ces features.\n",
    "\n",
    "On va de plus séparer le set d'entraînement afin d'obtenir un set de validation qui servira à contrôler les résultats de la cross validation, le set de test servant uniquement pour obtenir une note via Kaggle. Le travail suivant se fera ensuite à l'aide d'une sample du dataset, afin de réduire les temps de traitement de notre analyse et des cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b166a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Supprime les features avec trop de valeurs manquantes\n",
    "def empty_features(data) :\n",
    "    features = data.loc[:,data.isna().mean()>0.75].columns\n",
    "    data.drop(features, inplace=True, axis=1) #35 colonnes\n",
    "    return data\n",
    "\n",
    "# Sépare le dataset en 2 sets et prend un échantillon\n",
    "def sample_train_test(data) :\n",
    "    # Récupérer les sets train et test originaux\n",
    "    df_test = data[data['TARGET'].isna()].reset_index(drop=True)\n",
    "    df_train = data[~data['TARGET'].isna()].reset_index(drop=True) \n",
    "    \n",
    "    # Echantillonage\n",
    "    df_sample = df_train.groupby('TARGET').sample(frac=0.05, random_state=10).reset_index(drop=True)\n",
    "    \n",
    "    return df_test, df_train, df_sample\n",
    "\n",
    "# Renommer certaines colonnes avec des caractères non reconnus\n",
    "df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "# Appliquer les dernières fonctions\n",
    "df = empty_features(df)\n",
    "df_test, df_train, df_sample = sample_train_test(df)\n",
    "\n",
    "# On sépare le set de train pour créer un set de validation\n",
    "features_train = [x for x in df_train.columns if x not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV']]\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    df_sample[features_train], \n",
    "    df_sample['TARGET'], test_size=0.25, random_state=10, stratify=df_sample['TARGET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f7884",
   "metadata": {},
   "source": [
    "Maintenant qu'on a un dataset utilisable, on va pouvoir essayer divers modèles afin de comparer les résultats obtenus. On prendra ensuite le modèle le plus performant et on viendra utiliser des algorithmes de cross validation afin d'optimiser ses hyperparamètres.\n",
    "\n",
    "Cependant, pour comparer ces modèles, ainsi que pour optimiser les paramètres, il va nous falloir utiliser un scorer. Pour cela on va utiliser des scores habituels, tels que l'accuracy ou le F1 score, ou bien l'AUC car c'est le score utilisé dans la compétition Kaggle. On va aussi créer notre propre scorer, qu'on appelera score métier, qui viendra calculer les pertes engendrées par les différentes erreurs de prédiction.\n",
    "\n",
    "Pour le calcul de ce score, on considère qu'un Faux Négatif (C'est à dire un mauvais client prédit comme un bon client) coûte 10 fois plus cher qu'un Faux Positif (Bon client prédit comme mauvais client)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3c8d9ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Positive Correlations:\n",
      " BURO_MONTHS_BALANCE_MIN_MIN               0.073225\n",
      "CC_AMT_INST_MIN_REGULARITY_MEAN           0.073724\n",
      "DAYS_EMPLOYED                             0.074957\n",
      "BURO_DAYS_CREDIT_MIN                      0.075248\n",
      "BURO_CREDIT_ACTIVE_Active_MEAN            0.077356\n",
      "PREV_NAME_CONTRACT_STATUS_Refused_MEAN    0.077681\n",
      "DAYS_BIRTH                                0.078242\n",
      "CC_CNT_DRAWINGS_CURRENT_MEAN              0.082520\n",
      "CC_AMT_RECEIVABLE_PRINCIPAL_MEAN          0.086062\n",
      "CC_AMT_RECIVABLE_MEAN                     0.086478\n",
      "CC_AMT_TOTAL_RECEIVABLE_MEAN              0.086490\n",
      "CC_AMT_BALANCE_MEAN                       0.087177\n",
      "BURO_DAYS_CREDIT_MEAN                     0.089731\n",
      "CC_CNT_DRAWINGS_CURRENT_MAX               0.101389\n",
      "TARGET                                    1.000000\n",
      "Name: TARGET, dtype: float64\n",
      "\n",
      "Most Negative Correlations:\n",
      " EXT_SOURCE_3                              -0.178926\n",
      "EXT_SOURCE_2                              -0.160471\n",
      "EXT_SOURCE_1                              -0.155317\n",
      "BURO_MONTHS_BALANCE_SIZE_MEAN             -0.080193\n",
      "BURO_CREDIT_ACTIVE_Closed_MEAN            -0.079369\n",
      "PREV_CODE_REJECT_REASON_XAP_MEAN          -0.073938\n",
      "DAYS_EMPLOYED_PERC                        -0.067952\n",
      "ACTIVE_MONTHS_BALANCE_SIZE_MEAN           -0.065154\n",
      "PREV_NAME_CONTRACT_STATUS_Approved_MEAN   -0.063526\n",
      "CC_COUNT                                  -0.060481\n",
      "CC_NAME_CONTRACT_STATUS_Active_SUM        -0.059376\n",
      "CC_MONTHS_BALANCE_VAR                     -0.058817\n",
      "CLOSED_MONTHS_BALANCE_SIZE_MEAN           -0.058354\n",
      "NAME_EDUCATION_TYPE_Highereducation       -0.056593\n",
      "BURO_STATUS_C_MEAN_MEAN                   -0.055936\n",
      "Name: TARGET, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "correlations = df.corr()['TARGET'].sort_values()\n",
    "\n",
    "# Display correlations\n",
    "print('Most Positive Correlations:\\n', correlations.dropna().tail(15))\n",
    "print('\\nMost Negative Correlations:\\n', correlations.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "0e39d96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def score_metier(y_true,y_pred,seuil=0.5) :\n",
    "    y_true.reset_index(inplace=True,drop=True)\n",
    "    error = 0\n",
    "    for i in range (len(y_true)) :\n",
    "        # Si la probabilité est suffisante le prêt est refusé\n",
    "        if y_pred[i] > seuil :\n",
    "            #Prêt refusé alors que le client aurait remboursé = manque à gagner\n",
    "            if y_true[i] == 0:\n",
    "                error += 1\n",
    "        else :\n",
    "            #Prêt accepté alors que le client ne peut pas rembourser = perte d'argent\n",
    "            if y_true[i] == 1:\n",
    "                error += 10\n",
    "    return error\n",
    "\n",
    "metier_scorer = make_scorer(score_metier, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf8203e",
   "metadata": {},
   "source": [
    "Dans un objectif de comparaison, commençons en présentant un Classifier très simpliste. Le DummyClassifier peut utiliser plusieurs stratégies : toujours garder la même prédiction, stratifier en fonction des probabilités ou uniformiser les classes. Ces solutions sont dans tous les cas très basiques et ne vont pas nous offrir de très bons résultats, surtout au vu du déséquilibre au sein du dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "74211cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.92      0.92      3534\n",
      "         1.0       0.08      0.08      0.08       310\n",
      "\n",
      "    accuracy                           0.85      3844\n",
      "   macro avg       0.50      0.50      0.50      3844\n",
      "weighted avg       0.85      0.85      0.85      3844\n",
      "\n",
      "---------------------------------------------\n",
      "AUC Score :  0.49900962082625916\n",
      "---------------------------------------------\n",
      "Score métier :  3142\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "dummy = DummyClassifier(strategy=\"stratified\", random_state = 10)\n",
    "dummy.fit(x_train,y_train)\n",
    "probas = dummy.predict_proba(x_valid)\n",
    "print(classification_report(y_valid, probas[:,1], zero_division=0))\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"AUC Score : \",roc_auc_score(y_valid,probas[:,1]))\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"Score métier : \", score_metier(y_valid, probas[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3facaf",
   "metadata": {},
   "source": [
    "On pourrait maintenant travailler avec un classifier plus poussé, tel que LogisticRegression qui est un modèle linéaire. Cependant, il reste certaines valeurs manquantes dans notre dataset, et LogisticRegression ne peut fonctionner avec cela, on va donc commencer par imputer les valeurs manquantes dans le dataset.\n",
    "L'algorithme de LogisticRegression nous permet également de pallier au déséquilibre des classes grâce au paramètre *class_weight*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5e4688a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.96      0.94      3534\n",
      "         1.0       0.16      0.09      0.12       310\n",
      "\n",
      "    accuracy                           0.89      3844\n",
      "   macro avg       0.54      0.52      0.53      3844\n",
      "weighted avg       0.86      0.89      0.87      3844\n",
      "\n",
      "---------------------------------------------\n",
      "AUC Score :  0.566558044434708\n",
      "---------------------------------------------\n",
      "Score métier :  2967\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(strategy='median')\n",
    "\n",
    "x_train_imp = pd.DataFrame(imp.fit_transform(x_train), columns=x_train.columns)\n",
    "x_valid_imp = pd.DataFrame(imp.transform(x_valid), columns=x_train.columns)\n",
    "\n",
    "reg = LogisticRegression(class_weight='balanced', max_iter=300)\n",
    "reg.fit(x_train_imp,y_train)\n",
    "predict = reg.predict(x_valid_imp)\n",
    "probas = reg.predict_proba(x_valid_imp)\n",
    "print(classification_report(y_valid, predict, zero_division=0))\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"AUC Score : \",roc_auc_score(y_valid,probas[:,1]))\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"Score métier : \", score_metier(y_valid, probas[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25b61cd",
   "metadata": {},
   "source": [
    "On pourrait également tenter une autre approche pour résoudre le problème de déséquilibre de classes, en utilisant la librairie SMOTE. Smote est une fonction qui permet d'oversampler les classes sous-représentées, en créant de nouveaux individus proches des individus existants, et ainsi on fait travailler notre modèle directement sur ces données resamplés plutôt que d'intervenir au niveau du poids des classes dans le scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c6f5958f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.96      0.94      3534\n",
      "         1.0       0.15      0.08      0.10       310\n",
      "\n",
      "    accuracy                           0.89      3844\n",
      "   macro avg       0.53      0.52      0.52      3844\n",
      "weighted avg       0.86      0.89      0.87      3844\n",
      "\n",
      "---------------------------------------------\n",
      "AUC Score :  0.5636033371670591\n",
      "---------------------------------------------\n",
      "Score métier :  3001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=10, k_neighbors=3)\n",
    "\n",
    "x_train_bal, y_train_bal = smote.fit_resample(x_train_imp, y_train)\n",
    "\n",
    "\n",
    "reg = LogisticRegression(max_iter=300)\n",
    "reg.fit(x_train_bal,y_train_bal)\n",
    "predict = reg.predict(x_valid_imp)\n",
    "probas = reg.predict_proba(x_valid_imp)\n",
    "print(classification_report(y_valid, predict, zero_division=0))\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"AUC Score : \",roc_auc_score(y_valid,probas[:,1]))\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"Score métier : \", score_metier(y_valid, probas[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d2d1b",
   "metadata": {},
   "source": [
    "On voit qu'en appliquant cette méthode, on trouve un meilleur score AUC, mais le score métier n'est lui pas vraiment meilleur. On rappel cependant que l'on applique notre modèle avec les paramètres de bases et qu'on pourrait sûrement améliorer les performances de notre modèle en les modifiant.\n",
    "\n",
    "Avant de faire cela, comparons maintenant avec un 3e modèle de classification, LightGBM. LightGBM traite directement les données même avec des valeurs manquantes et des déséquilibres, on applique donc tout d'abord le LGBM sur x_train simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6c3c06d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035879 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      1.00      0.96      3534\n",
      "         1.0       0.48      0.04      0.08       310\n",
      "\n",
      "    accuracy                           0.92      3844\n",
      "   macro avg       0.70      0.52      0.52      3844\n",
      "weighted avg       0.89      0.92      0.89      3844\n",
      "\n",
      "---------------------------------------------\n",
      "AUC Score :  0.7572795151249613\n",
      "---------------------------------------------\n",
      "Score métier :  2984\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgbm = lgb.LGBMClassifier(verbosity=0)\n",
    "\n",
    "lgbm.fit(x_train,y_train)\n",
    "predict = lgbm.predict(x_valid)\n",
    "probas = lgbm.predict_proba(x_valid)\n",
    "print(classification_report(y_valid, predict, zero_division=0))\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"AUC Score : \",roc_auc_score(y_valid,probas[:,1]))\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"Score métier : \", score_metier(y_valid, probas[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7924f3dc",
   "metadata": {},
   "source": [
    "Faisons maintenant un essai en utilisant le paramètre *class_weight* de LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0f48eb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037951 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.92      0.93      3534\n",
      "         1.0       0.27      0.35      0.30       310\n",
      "\n",
      "    accuracy                           0.87      3844\n",
      "   macro avg       0.61      0.63      0.62      3844\n",
      "weighted avg       0.89      0.87      0.88      3844\n",
      "\n",
      "---------------------------------------------\n",
      "AUC Score :  0.7498530405097031\n",
      "---------------------------------------------\n",
      "Score métier :  2317\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgbm = lgb.LGBMClassifier(verbosity=0, class_weight='balanced')\n",
    "\n",
    "lgbm.fit(x_train,y_train)\n",
    "predict = lgbm.predict(x_valid)\n",
    "probas = lgbm.predict_proba(x_valid)\n",
    "print(classification_report(y_valid, predict, zero_division=0))\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"AUC Score : \",roc_auc_score(y_valid,probas[:,1]))\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"Score métier : \", score_metier(y_valid, probas[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c538c504",
   "metadata": {},
   "source": [
    "Enfin, essayons en entraînant le modèle grâce au set d'entraînement resamplé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5ab9b000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070560 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.99      0.96      3534\n",
      "         1.0       0.47      0.05      0.09       310\n",
      "\n",
      "    accuracy                           0.92      3844\n",
      "   macro avg       0.70      0.52      0.53      3844\n",
      "weighted avg       0.89      0.92      0.89      3844\n",
      "\n",
      "---------------------------------------------\n",
      "AUC Score :  0.7390930500027384\n",
      "---------------------------------------------\n",
      "Score métier :  2958\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "smote = SMOTE(random_state=10, k_neighbors=4)\n",
    "\n",
    "x_train_bal, y_train_bal = smote.fit_resample(x_train_imp, y_train)\n",
    "\n",
    "lgbm = lgb.LGBMClassifier(verbosity=0)\n",
    "\n",
    "lgbm.fit(x_train_bal,y_train_bal)\n",
    "predict = lgbm.predict(x_valid)\n",
    "probas = lgbm.predict_proba(x_valid)\n",
    "print(classification_report(y_valid, predict, zero_division=0))\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"AUC Score : \",roc_auc_score(y_valid,probas[:,1]))\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"Score métier : \", score_metier(y_valid, probas[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b78244b",
   "metadata": {},
   "source": [
    "On voit avec ces 3 résultats que les deux meilleurs méthodes sont en gardant le dataset de base, ou en utilisant smote sur le set de train.\n",
    "\n",
    "Il apparaît en tous cas largement que LightGBM est le meilleur modèle pour notre problème, c'est donc ce modèle qu'on va maintenant essayé d'optimiser en faisant varier les paramètres.\n",
    "\n",
    "Pour utiliser la cross validation avec notre resampling, on va devoir créer un Pipeline via la librairie **ImbLearn**. Ce Pipeline va nous permettre d'appliquer le resampling sur x_train au sein de la cross validation, en appliquant smote uniquement à l'échantillon de train et pas celui de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3c9a1be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Search - done in 5493s\n",
      "Score métier -1797.0\n",
      "AUC 0.6144640998959416\n",
      "{'sampling__k_neighbors': 5, 'classification__reg_lambda': 0.061224489795918366, 'classification__reg_alpha': 0.36734693877551017, 'classification__num_leaves': 5, 'classification__min_child_samples': 40, 'classification__learning_rate': 0.004999999999999999, 'classification__class_weight': 'balanced'}\n"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'sampling__k_neighbors' : [3,4,5,6],\n",
    "    'classification__num_leaves': list(range(5,75,3)),\n",
    "    'classification__learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.1),base = 10, num = 100)),\n",
    "    'classification__min_child_samples': list(range(10,50,2)),\n",
    "    'classification__reg_alpha': list(np.linspace(0, 1)),\n",
    "    'classification__reg_lambda': list(np.linspace(0, 1)),\n",
    "    'classification__class_weight' : ['balanced', None]\n",
    "                          }\n",
    "\n",
    "model = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=10)),\n",
    "        ('classification', lgb.LGBMClassifier())\n",
    "    ])\n",
    "\n",
    "with mlflow.start_run() :\n",
    "    opt = RandomizedSearchCV(\n",
    "         model,\n",
    "        param_grid,\n",
    "         n_iter=500,\n",
    "        cv = 5,\n",
    "         random_state=10,\n",
    "        scoring=metier_scorer\n",
    "     )\n",
    "\n",
    "    with timer('Random Search') :\n",
    "        # On utilise _imp car Smote nécessite \n",
    "        opt.fit(x_train_imp, y_train)\n",
    "        \n",
    "    mlflow.log_param(\"Hyperparams\", opt.best_params_)\n",
    "    mlflow.log_metric(\"Score Metier\", opt.best_score_)\n",
    "    mlflow.log_metric('AUC', roc_auc_score(y_valid, opt.best_estimator_.predict_proba(x_valid_imp)[:,1]))\n",
    "    print(\"Score métier\", opt.best_score_)\n",
    "    print(\"AUC\",roc_auc_score(y_valid, opt.best_estimator_.predict_proba(x_valid_imp)[:,1]) )\n",
    "    print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "1ce2a048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Search - done in 12s\n",
      "-1803.8\n",
      "{}\n",
      "AUC 0.7418250360552787\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "opt = GridSearchCV(\n",
    "     model,\n",
    "    param_grid={},\n",
    "    cv = 5,\n",
    "    scoring=metier_scorer\n",
    " )\n",
    "\n",
    "with timer('Random Search') :\n",
    "    opt.fit(x_train_imp, y_train)\n",
    "\n",
    "\n",
    "print(opt.best_score_)\n",
    "print(opt.best_params_)\n",
    "print(\"AUC\",roc_auc_score(y_valid, opt.best_estimator_.predict_proba(x_valid_imp)[:,1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "2744bf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Search - done in 1189s\n",
      "Score métier -1804.6\n",
      "AUC 0.7601109954908081\n",
      "{'sampling__k_neighbors': 4, 'classification__reg_lambda': 0.04081632653061224, 'classification__reg_alpha': 0.7551020408163265, 'classification__num_leaves': 20, 'classification__min_child_samples': 22, 'classification__learning_rate': 0.08595901184719835, 'classification__class_weight': 'balanced'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'sampling__k_neighbors' : [3,4,5,6],\n",
    "    'classification__num_leaves': list(range(5,75,3)),\n",
    "    'classification__learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.1),base = 10, num = 100)),\n",
    "    'classification__min_child_samples': list(range(10,50,2)),\n",
    "    'classification__reg_alpha': list(np.linspace(0, 1)),\n",
    "    'classification__reg_lambda': list(np.linspace(0, 1)),\n",
    "    'classification__class_weight' : ['balanced', None]\n",
    "                          }\n",
    "\n",
    "model = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=10)),\n",
    "        ('classification', lgb.LGBMClassifier())\n",
    "    ])\n",
    "\n",
    "with mlflow.start_run() :\n",
    "    opt = RandomizedSearchCV(\n",
    "         model,\n",
    "        param_grid,\n",
    "         n_iter=100,\n",
    "        cv = 5,\n",
    "         random_state=10,\n",
    "        scoring=metier_scorer\n",
    "     )\n",
    "\n",
    "    with timer('Random Search') :\n",
    "        # On utilise _imp car Smote ne marche pas avec les valeurs manquantes \n",
    "        opt.fit(x_train_imp, y_train)\n",
    "        \n",
    "    mlflow.log_param(\"Hyperparams\", opt.best_params_)\n",
    "    mlflow.log_metric(\"Score Metier\", opt.best_score_)\n",
    "    mlflow.log_metric('AUC', roc_auc_score(y_valid, opt.best_estimator_.predict_proba(x_valid_imp)[:,1]))\n",
    "    print(\"Score métier\", opt.best_score_)\n",
    "    print(\"AUC\",roc_auc_score(y_valid, opt.best_estimator_.predict_proba(x_valid_imp)[:,1]) )\n",
    "    print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "ab8d15e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_est = opt.best_estimator_\n",
    "best_params = opt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "ff603501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metier 3032\n"
     ]
    }
   ],
   "source": [
    "print(\"metier\",score_metier(y_valid, opt.best_estimator_.predict_proba(x_valid)[:,1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "706c4e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Search - done in 7s\n",
      "-2120.0\n",
      "0.1\n",
      "Random Search - done in 7s\n",
      "-2120.0\n",
      "0.25\n",
      "Random Search - done in 6s\n",
      "-1766.6\n",
      "0.4\n",
      "Random Search - done in 7s\n",
      "-1797.0\n",
      "0.5\n",
      "Random Search - done in 7s\n",
      "-1862.0\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "seuils = [0.1,0.25,0.4,0.5,0.75]\n",
    "for lim in seuils : \n",
    "    metier_scorer = make_scorer(score_metier, greater_is_better=False, needs_proba=True, **{'seuil':lim})\n",
    "    with mlflow.start_run() :\n",
    "        opt = GridSearchCV(\n",
    "             model,\n",
    "            param_grid={'sampling__k_neighbors': [5],\n",
    "                   'classification__min_child_samples': [40],\n",
    "                   'classification__reg_alpha': [0.36734693877551017],\n",
    "                    'classification__reg_lambda': [0.061224489795918366],\n",
    "                   'classification__learning_rate': [0.004999999999999999],\n",
    "                   'classification__num_leaves': [5]},\n",
    "            cv = 5,\n",
    "            scoring=metier_scorer\n",
    "         )\n",
    "\n",
    "        with timer('Random Search') :\n",
    "            opt.fit(x_train_imp, y_train)\n",
    "            \n",
    "        print(opt.best_score_)\n",
    "        print(lim)\n",
    "        \n",
    "        mlflow.log_param(\"Hyperparams\", opt.best_params_)\n",
    "        mlflow.log_metric(\"AUC\", opt.best_score_)\n",
    "\n",
    "metier_scorer = make_scorer(score_metier, greater_is_better=False, needs_proba=True, **{'seuil':0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "091db053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Search - done in 7s\n",
      "-1797.0\n",
      "{'classification__learning_rate': 0.004999999999999999, 'classification__min_child_samples': 40, 'classification__num_leaves': 5, 'classification__reg_alpha': 0.36734693877551017, 'classification__reg_lambda': 0.061224489795918366, 'sampling__k_neighbors': 5}\n",
      "AUC 0.6301111780491813\n"
     ]
    }
   ],
   "source": [
    "opt = GridSearchCV(\n",
    "     model,\n",
    "    param_grid={'sampling__k_neighbors': [5],\n",
    "                   'classification__min_child_samples': [40],\n",
    "                   'classification__reg_alpha': [0.36734693877551017],\n",
    "                    'classification__reg_lambda': [0.061224489795918366],\n",
    "                   'classification__learning_rate': [0.004999999999999999],\n",
    "                   'classification__num_leaves': [5]},\n",
    "    cv = 5,\n",
    "    scoring=metier_scorer\n",
    " )\n",
    "\n",
    "with timer('Random Search') :\n",
    "    opt.fit(x_train_imp, y_train)\n",
    "\n",
    "\n",
    "print(opt.best_score_)\n",
    "print(opt.best_params_)\n",
    "print(\"AUC\",roc_auc_score(y_valid, opt.best_estimator_.predict_proba(x_valid)[:,1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "d4fc77f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3013"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_metier(y_valid, opt.best_estimator_.predict_proba(x_valid)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "6211d3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>...</th>\n",
       "      <th>CC_NAME_CONTRACT_STATUS_Signed_MAX</th>\n",
       "      <th>CC_NAME_CONTRACT_STATUS_Signed_MEAN</th>\n",
       "      <th>CC_NAME_CONTRACT_STATUS_Signed_SUM</th>\n",
       "      <th>CC_NAME_CONTRACT_STATUS_Signed_VAR</th>\n",
       "      <th>CC_NAME_CONTRACT_STATUS_nan_MIN</th>\n",
       "      <th>CC_NAME_CONTRACT_STATUS_nan_MAX</th>\n",
       "      <th>CC_NAME_CONTRACT_STATUS_nan_MEAN</th>\n",
       "      <th>CC_NAME_CONTRACT_STATUS_nan_SUM</th>\n",
       "      <th>CC_NAME_CONTRACT_STATUS_nan_VAR</th>\n",
       "      <th>CC_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>1129500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307502</th>\n",
       "      <td>456251</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>157500.0</td>\n",
       "      <td>254700.0</td>\n",
       "      <td>27558.0</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307503</th>\n",
       "      <td>456252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>269550.0</td>\n",
       "      <td>12001.5</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307504</th>\n",
       "      <td>456253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>153000.0</td>\n",
       "      <td>677664.0</td>\n",
       "      <td>29979.0</td>\n",
       "      <td>585000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307505</th>\n",
       "      <td>456254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>171000.0</td>\n",
       "      <td>370107.0</td>\n",
       "      <td>20205.0</td>\n",
       "      <td>319500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307506</th>\n",
       "      <td>456255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>157500.0</td>\n",
       "      <td>675000.0</td>\n",
       "      <td>49117.5</td>\n",
       "      <td>675000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>307507 rows × 762 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SK_ID_CURR  TARGET  CODE_GENDER  FLAG_OWN_CAR  FLAG_OWN_REALTY  \\\n",
       "0           100002     1.0            0             0                0   \n",
       "1           100003     0.0            1             0                1   \n",
       "2           100004     0.0            0             1                0   \n",
       "3           100006     0.0            1             0                0   \n",
       "4           100007     0.0            0             0                0   \n",
       "...            ...     ...          ...           ...              ...   \n",
       "307502      456251     0.0            0             0                1   \n",
       "307503      456252     0.0            1             0                0   \n",
       "307504      456253     0.0            1             0                0   \n",
       "307505      456254     1.0            1             0                0   \n",
       "307506      456255     0.0            1             0                1   \n",
       "\n",
       "        CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0                  0          202500.0    406597.5      24700.5   \n",
       "1                  0          270000.0   1293502.5      35698.5   \n",
       "2                  0           67500.0    135000.0       6750.0   \n",
       "3                  0          135000.0    312682.5      29686.5   \n",
       "4                  0          121500.0    513000.0      21865.5   \n",
       "...              ...               ...         ...          ...   \n",
       "307502             0          157500.0    254700.0      27558.0   \n",
       "307503             0           72000.0    269550.0      12001.5   \n",
       "307504             0          153000.0    677664.0      29979.0   \n",
       "307505             0          171000.0    370107.0      20205.0   \n",
       "307506             0          157500.0    675000.0      49117.5   \n",
       "\n",
       "        AMT_GOODS_PRICE  ...  CC_NAME_CONTRACT_STATUS_Signed_MAX  \\\n",
       "0              351000.0  ...                                 NaN   \n",
       "1             1129500.0  ...                                 NaN   \n",
       "2              135000.0  ...                                 NaN   \n",
       "3              297000.0  ...                                 0.0   \n",
       "4              513000.0  ...                                 NaN   \n",
       "...                 ...  ...                                 ...   \n",
       "307502         225000.0  ...                                 NaN   \n",
       "307503         225000.0  ...                                 NaN   \n",
       "307504         585000.0  ...                                 NaN   \n",
       "307505         319500.0  ...                                 NaN   \n",
       "307506         675000.0  ...                                 NaN   \n",
       "\n",
       "        CC_NAME_CONTRACT_STATUS_Signed_MEAN  \\\n",
       "0                                       NaN   \n",
       "1                                       NaN   \n",
       "2                                       NaN   \n",
       "3                                       0.0   \n",
       "4                                       NaN   \n",
       "...                                     ...   \n",
       "307502                                  NaN   \n",
       "307503                                  NaN   \n",
       "307504                                  NaN   \n",
       "307505                                  NaN   \n",
       "307506                                  NaN   \n",
       "\n",
       "        CC_NAME_CONTRACT_STATUS_Signed_SUM  \\\n",
       "0                                      NaN   \n",
       "1                                      NaN   \n",
       "2                                      NaN   \n",
       "3                                      0.0   \n",
       "4                                      NaN   \n",
       "...                                    ...   \n",
       "307502                                 NaN   \n",
       "307503                                 NaN   \n",
       "307504                                 NaN   \n",
       "307505                                 NaN   \n",
       "307506                                 NaN   \n",
       "\n",
       "        CC_NAME_CONTRACT_STATUS_Signed_VAR  CC_NAME_CONTRACT_STATUS_nan_MIN  \\\n",
       "0                                      NaN                              NaN   \n",
       "1                                      NaN                              NaN   \n",
       "2                                      NaN                              NaN   \n",
       "3                                      0.0                              0.0   \n",
       "4                                      NaN                              NaN   \n",
       "...                                    ...                              ...   \n",
       "307502                                 NaN                              NaN   \n",
       "307503                                 NaN                              NaN   \n",
       "307504                                 NaN                              NaN   \n",
       "307505                                 NaN                              NaN   \n",
       "307506                                 NaN                              NaN   \n",
       "\n",
       "        CC_NAME_CONTRACT_STATUS_nan_MAX  CC_NAME_CONTRACT_STATUS_nan_MEAN  \\\n",
       "0                                   NaN                               NaN   \n",
       "1                                   NaN                               NaN   \n",
       "2                                   NaN                               NaN   \n",
       "3                                   0.0                               0.0   \n",
       "4                                   NaN                               NaN   \n",
       "...                                 ...                               ...   \n",
       "307502                              NaN                               NaN   \n",
       "307503                              NaN                               NaN   \n",
       "307504                              NaN                               NaN   \n",
       "307505                              NaN                               NaN   \n",
       "307506                              NaN                               NaN   \n",
       "\n",
       "        CC_NAME_CONTRACT_STATUS_nan_SUM  CC_NAME_CONTRACT_STATUS_nan_VAR  \\\n",
       "0                                   NaN                              NaN   \n",
       "1                                   NaN                              NaN   \n",
       "2                                   NaN                              NaN   \n",
       "3                                   0.0                              0.0   \n",
       "4                                   NaN                              NaN   \n",
       "...                                 ...                              ...   \n",
       "307502                              NaN                              NaN   \n",
       "307503                              NaN                              NaN   \n",
       "307504                              NaN                              NaN   \n",
       "307505                              NaN                              NaN   \n",
       "307506                              NaN                              NaN   \n",
       "\n",
       "        CC_COUNT  \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "2            NaN  \n",
       "3            6.0  \n",
       "4            NaN  \n",
       "...          ...  \n",
       "307502       NaN  \n",
       "307503       NaN  \n",
       "307504       NaN  \n",
       "307505       NaN  \n",
       "307506       NaN  \n",
       "\n",
       "[307507 rows x 762 columns]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "0621cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('df_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85880a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
